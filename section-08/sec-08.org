#+INCLUDE: ../orgpreamble.org

*Asymptopia is the best -topia* \hfill
*ARE212*: Section 08 \\ \hline \bigskip

In this section we'll briefly clarify some concepts from last time, and then we'll build up a simulation in =R= to show why living in asymptotic world is great. With any remaining time we'll look into what happens when your errors have finite mean but infinite variance. First though, some brief notes on the problem set you all turned in Monday.

* Problem set 2 retrospective
** Disturbances vs. residuals
Remember, we /never/ observe $\eps$, the vector of disturbances. We hope to approximate it with our residuals vector, $\e$. We /assume/ that $E[\eps|X] = 0$, but the fact that $E[\e|X] = 0$ is by construction of OLS. This is a subtle distinction, but an important one. The first is very difficult to prove with certainty, but the second is always true (as long as we have an intercept).
** =pairs= and =ggpairs=
Many of you constructed scatterplots of your data to find patterns. This is good. You can do this quite easily using either =pairs()= or =ggpairs()=. For example:
#+begin_src R :results output graphics :file inserts/fig0.png :width 800 :height 500 :session :tangle yes :exports both
library(ggplot2)
library(GGally)
ggpairs(iris, title = "Scatterplot matrix of variables in iris dataset")
#+end_src

#+RESULTS:
[[file:inserts/fig0.png]]
** =J= in an F-test
Counting the number of restrictions, =J=, in an F-test, can be tricky. Suppose we are testing whether the mean $y$ values for group dummy variables are statistically different from each other in the following regression:

$$ y = \beta_1 G_1 + \beta_2 G_2 + \beta_3 G_3 + \varepsilon $$

$G_i$ is a dummy variable for group $i$. We omit the intercept, so the covariates we get from OLS are the mean values for each group. To test equality across all three groups using an F test, we only need to set $\beta_1 = \beta_2$ and $\beta_1 = \beta_3$. This is two restrictions. More generally, you only need $j-1$ restrictions to test equality across $j$ coefficients.
** Standard errors!
This is going to sound peevish, but many problem sets contained estimates without standard errors! Some of them weren't computable without using the delta method (which was not the focus of the problem set), but many of the neglected estimates were run-of-the-mill OLS coefficients, for which you should know the standard error formula cold by now. Remember, coefficients without standard errors are very nearly meaningless.
* The difference between prediction and confidence intervals

Last week I punted on demonstrating the difference between prediction and confidence intervals. As it turns out, we mostly already showed the difference, but I didn't label them correctly. To remind you, we had the following population model:
$$ \y = \X \b + \eps $$
We computed that the variance of the /individual prediction/, $V(\hat{y}^0 - y^0)$ and the variance of the /mean prediction/, $V(\hat{y}^0)$, and showed they are different estimates[fn:: Simon Jackman has a nice, complete exploration of this topic here: http://jackman.stanford.edu/classes/350B/07/predictionforWeb.pdf.]:

\begin{align}
V(\hat{y}^0 - y^0) &= \sigs( 1 + \X^0 (\Xp \X)^{-1} \Xp^0 ) \label{vpe} \\
V(\hat{y}^0) &= \sigs(\X^0 (\Xp \X)^{-1} \Xp^0 ) \label{vp}
\end{align}

It turns out that when we compute the /prediction interval/ (i.e. the interval around the individual prediction), we use (\ref{vpe}), while when we compute the /confidence interval/ (i.e. the interval around the mean prediction), we use (\ref{vp}). Hopefully this terminology makes sense to you. The intution is that the prediction interval tells us the error we should expect when we predict /new/ values of $y$, whereas the confidence interval tells us the confidence we should have in the estimate $\hat{y}^0 = \X^0 \b$. \\

The source of the confusion was that =predict()=, the canned =R= command, returns the standard error of the prediction (i.e. the square root of (\ref{vp})), when we set =se.fit = TRUE=. But we can also use =interval= setting to return both types of intervals! Let's dive into =R= to demonstrate a key fact about the relationship between prediction and confidence intervals. First, we'll build up our simulated data:

#+begin_src R :results output :exports both :tangle yes :session
OLS <- function(y,X) { b <- solve(t(X) %*% X) %*% t(X) %*% y }
set.seed(42)
n <- 200
X <- cbind(1, rnorm(n))
beta <- c(2, 3)
eps <- rnorm(n)
y <- X %*% beta + eps
#+end_src

#+RESULTS:

Now we can construct both the standard errors for the prediction interval and the confidence interval. We'll name them intuitively.

#+begin_src R :results output :exports both :tangle yes :session
b <- OLS(y,X)
pred.y <- X %*% b
e <- y - pred.y
n <- nrow(X); k <- ncol(X)
s2 <- as.vector(t(e) %*% e / (n - k))
XpXinv <- solve(t(X) %*% X)
pred.se <- sqrt(diag(s2 * (1 + X %*% XpXinv %*% t(X))))
conf.se <- sqrt(diag(s2 * (0 + X %*% XpXinv %*% t(X))))
(cbind(head(pred.se), head(conf.se)))
#+end_src

#+RESULTS:
:           [,1]       [,2]
: [1,] 0.9536325 0.11724104
: [2,] 0.9494816 0.07645859
: [3,] 0.9491422 0.07212048
: [4,] 0.9498495 0.08089918
: [5,] 0.9492266 0.07322351
: [6,] 0.9487766 0.06713905

As with our coefficient estimates, we'll calculate the confidence intervals using the 95% critical value from a $t_{n-k}$ distribution.

#+begin_src R :results output :exports both :tangle yes :session
crit.t <- qt(0.975,n-k)
confint.t <- cbind(pred.y - crit.t * conf.se, pred.y + crit.t * conf.se)
predint.t <- cbind(pred.y - crit.t * pred.se, pred.y+ crit.t * pred.se)
#+end_src

#+RESULTS:

We could have also calculated and checked the intervals using =predict()=. But they're right. Trust me. Now we'll use =ggplot2='s =geom_ribbon()= function to plot them.

#+begin_src R :results output graphics :file inserts/fig1.png :width 800 :height 500 :session :tangle yes :exports both
draw.df <- data.frame(X[ ,2], y, pred.y, confint.t, predint.t)
names(draw.df) <- c("x", "y", "pred.y","conf.low","conf.high","pred.low","pred.high")
library(ggplot2)
(g <- ggplot(data = draw.df, aes(x = x, y = y)) + ggtitle("Prediction vs. Confidence Intervals") +
  geom_ribbon(aes(ymin = pred.low, ymax = pred.high), fill = "chartreuse1", linetype = 0) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = "coral1", linetype = 0) +
  geom_point(colour = "darkslategray", size = 0.2))
#+end_src

#+RESULTS:
[[file:inserts/fig1.png]]

The chartreuse (outer) area is the prediction interval, while the coral (inner) area is the confidence area. Clearly, the confidence interval is always a subset of the prediction interval. This is true in general. We can see this by looking at our formulas and noting that $\sigs > 0 \Rightarrow V(\hat{y}^0 - y^0) > V(\hat{y}^0)$.

* The glory of asymptopia
Finally, the main event. In this section, we'll verify in code what we showed in section 4.2 in lecture. We'll demonstrate that, given linearity, population orthogonality, and our asymptotic full rank condition, no matter what distribution on the disturbances, the estimated $\b$ will be distributed normal as $N \rightarrow \infty$. \\

We'll be doing another simulation exercise. Let's get started. First, we'll define our true population of interest, the size of sample for each draw, and the number of draws.

#+begin_src R :results output :exports both :tangle yes :session
pop.n <- 10000
n <- 500
draws <- 1000
pop.X <- cbind(1, runif(pop.n,0,20))
#+end_src

#+RESULTS:

Next we want to define some errors. To make it interesting, we'll do this whole exercise with four different sets of errors. The sets will be distributed normal, uniform, poisson, and finally a crazy bimodal gamma distribution that I made up. This is going to be awesome.

#+begin_src R :results output :exports both :tangle yes :session
bimodalDistFunc <- function (n, cpct, mu1, mu2, sig1, sig2) {
  d0 <- rgamma(n, 1) + 3
  d1 <- rgamma(n, 1)
  flag <- rbinom(n, size = 1, prob = cpct)
  d <- d1 * (1 - flag) + d0 * flag
}

pop.eps.0 <- rnorm(pop.n)
pop.eps.1 <- runif(pop.n, -5, 5)
pop.eps.2 <- rpois(pop.n, 1) - 1
pop.eps.3 <- bimodalDistFunc(n=pop.n, cpct = 0.7, mu1 = 2, mu2 = 10, sig1 = 1, sig2 = 2) - 3.1
#+end_src

#+RESULTS:

It's important to note that I rigged these epsilons so that they all have mean zero:

#+begin_src R :results output :exports both :tangle yes :session
c(mean(pop.eps.0), mean(pop.eps.1), mean(pop.eps.2), mean(pop.eps.3))
#+end_src

#+RESULTS:
: [1] -0.005288162  0.006531189  0.027400000 -0.003300807

This isn't strictly necessary, but it makes our lives easier down the road. Now we'll create a data frame with our errors that we can easily graph using =ggplot2=. This takes a little doing --- we need to stack the errors on top of each other and add a factor variable for the type of error they are. Factor variables are just categorical variables in =R=.

#+begin_src R :results output :exports both :tangle yes :session
pop.eps <- c(pop.eps.0, pop.eps.1, pop.eps.2, pop.eps.3)
pop.eps.type <- c(rep("Normal",pop.n), rep("Uniform", pop.n),
                 rep("Poisson", pop.n), rep("Bimodal Gamma", pop.n))
class(pop.eps.type)
pop.eps.type <- factor(pop.eps.type, levels = c("Normal", "Uniform",
                                                "Poisson", "Bimodal Gamma"))
levels(pop.eps.type)
class(pop.eps.type)
pop.eps.df <- data.frame(pop.eps, pop.eps.type)
#+end_src

#+RESULTS:
: [1] "character"
: [1] "Normal"        "Uniform"       "Poisson"       "Bimodal Gamma"
: [1] "factor"

Now that we've set all of this up, why not see what these crazy errors look like?

#+begin_src R :results output graphics :file inserts/fig2.png :width 800 :height 500 :session :tangle yes :exports both
(g <- ggplot(data = pop.eps.df, aes(x = pop.eps)) +
        geom_histogram(aes(y=..density..)) +
        facet_wrap( ~ pop.eps.type, ncol=2, scales = "free"))
#+end_src

#+RESULTS:
[[file:inserts/fig2.png]]

Cool. The last three all violate our normality assumption pretty egregiously. Let's see how they do... in *asymptopia*! \\

Now we'll define our population outcome variables, one for each set of errors:
#+begin_src R :results output :exports both :tangle yes :session
beta <- c(4,2)
pop.y.0 <- pop.X %*% beta + pop.eps.0
pop.y.1 <- pop.X %*% beta + pop.eps.1
pop.y.2 <- pop.X %*% beta + pop.eps.2
pop.y.3 <- pop.X %*% beta + pop.eps.3
#+end_src

#+RESULTS:

We also need to build a function that returns OLS estimates for a given set of population variables. =getb= will be that function, and the code it in should be familiar from section 6. As we did then, we'll randomply sample indices (without replacement) and then pull the corresponding data from =pop.x= and =pop.y=.

#+begin_src R :results output :exports both :tangle yes :session
getb <- function(draw, pop.n, pop.y, pop.X) {
  indices <- sample(1:pop.n, n)
  y <- pop.y[indices]
  X <- pop.X[indices, ]
  b <- OLS(y,X)
  return(b)
}
#+end_src

#+RESULTS:

The code above won't win any points for efficiency, since we're passing the whole population every time, but it does generalize pretty well. In fact, we'll only modify the population of =y= that we pass.

#+begin_src R :results output :exports both :tangle yes :session
blist.0 <- t(sapply(1:draws, getb, pop.n = pop.n, pop.y = pop.y.0, pop.X = pop.X))
blist.1 <- t(sapply(1:draws, getb, pop.n = pop.n, pop.y = pop.y.1, pop.X = pop.X))
blist.2 <- t(sapply(1:draws, getb, pop.n = pop.n, pop.y = pop.y.2, pop.X = pop.X))
blist.3 <- t(sapply(1:draws, getb, pop.n = pop.n, pop.y = pop.y.3, pop.X = pop.X))
cbind(head(blist.0),head(blist.1),head(blist.2),head(blist.3))
#+end_src

#+RESULTS:
:          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]
: [1,] 3.950786 2.002674 3.776555 2.008685 4.237680 1.986850 4.243113 1.987808
: [2,] 3.902267 2.009341 4.142603 1.974937 3.937415 1.999047 4.129825 1.998343
: [3,] 3.942421 2.004543 3.910602 1.999602 3.987099 2.004227 4.206357 1.988775
: [4,] 3.811743 2.011721 4.235077 1.949349 3.966318 2.015415 3.970238 2.002788
: [5,] 4.091621 1.987538 4.210565 1.993504 4.025247 1.998070 4.278379 1.977353
: [6,] 4.009503 1.996708 3.943357 2.001888 4.036146 2.010304 3.984694 2.000783

This looks pretty good so far. The odd columns are our point estimates for $\alpha$, the intercept, and the even columns are our estimates for $\beta_1$, the coefficient estimate. From now on we'll focus on $\beta_1$. We've generated a huge amount of these point estimates, but now we'd like to see if they're truly distributed normaly. To do this, we'll create a function that graphs each of them.

#+begin_src R :results output :exports both :tangle yes :session
make.plots <- function(blist) {
  blist.df <- data.frame(blist)
  names(blist.df) <- c("alpha", "beta_1")
  estmean <- mean(blist.df$beta_1)
  estsd <- sd(blist.df$beta_1)
  g <- ggplot(data = blist.df, aes(x = beta_1)) +
   geom_histogram(aes(y=..density..), fill="blue", colour="white", alpha=0.3) +
   geom_density(colour = "black", size = 1, linetype = "dashed") +
   stat_function(geom="line", fun=dnorm, arg=list(mean = estmean, sd = estsd),
     colour = "red", size = 1, linetype = "dashed")
  return(g)
}

g0 <- make.plots(blist.0) + ggtitle("Normal errors")
g1 <- make.plots(blist.1) + ggtitle("Uniform errors")
g2 <- make.plots(blist.2) + ggtitle("Poisson errors")
g3 <- make.plots(blist.3) + ggtitle("Bimodal gamma errors")
#+end_src

#+RESULTS:

Our function =makehist= returns a graph a histogram, smoothed density, and a normal curve with the same mean and standard deviation as the data, so we just run it 4 times to get our 4 graphs (one for each list of our estimated $b$ values). Next, we'll use the =gridExtra= package (which you may need to install) to display the graphs together, just as we did with our errors. Note that we use =gridExtra= and not the =facets= command as we did before just because =facets= wasn't displaying the normal distribution correctly. What we'd like to see is that our smoothed density function and histogram look roughly similar to the normal distribution. Cross your fingers!

#+begin_src R :results output graphics :file inserts/fig3.png :width 800 :height 500 :session :tangle yes :exports both
library(gridExtra)
grid.arrange(g0, g1, g2, g3, ncol = 2)
#+end_src

#+RESULTS:
[[file:inserts/fig2.png]]

Hey, these actually look pretty good! But this isn't a rigorous test. There are actually statistical methods available to test whether a distribution is normal or not. We'll use one called the Shapiro-Wilk's test of normality. Unfortunately, the computation of Shapiro-Wilk's is a bit complicated, so we'll be leaning on a canned command here to perform it. There are other, simpler, tests (like Kolmogorov-Smirnov), but I like Shapiro-Wilk's because it's meant specifically for normal distributions. \\

First, we'll verify that the test works by testing it against a normal distribution and a non-normal distribution.

#+begin_src R :results output :exports both :tangle yes :session
(cbind(shapiro.test(rnorm(draws))[1:2], shapiro.test(rpois(draws,10))[1:2]))
#+end_src

#+RESULTS:
:           [,1]      [,2]
: statistic 0.9975899 0.9871238
: p.value   0.1498872 1.070178e-07

Great! The null is that the distribution is distributed normal, so we fail to reject (at any reasonable level) the null for the normal distribution and reject the null with a great deal of confidence for the poisson distribution. Now let's try it on our estimated coefficients:

#+begin_src R :results output :exports both :tangle yes :session
(cbind(
  shapiro.test(blist.0[ ,2])[1:2],
  shapiro.test(blist.1[ ,2])[1:2],
  shapiro.test(blist.2[ ,2])[1:2],
  shapiro.test(blist.3[ ,2])[1:2]))
#+end_src

#+RESULTS:
:           [,1]      [,2]      [,3]      [,4]
: statistic 0.9974901 0.9983627 0.9981639 0.9988518
: p.value   0.127683  0.4682959 0.3581798 0.7879152

Great! We fail to reject the null for all three distributions, so we can feel some confidence that our coefficients are truly distributed normal! All is well in asymptopia!

* Pareto distribution?
What about a probability distribution with finite mean, infinite variance? For the record, this violates our assumptions. For fun, let's add some zeroes to =pop.n=, =n=, and =draws=.
#+begin_src R :results output :exports both :tangle yes :session
library(VGAM)
pop.n <- 100000
n <- 5000
draws <- 10000
pop.X <- cbind(1, runif(pop.n,0,20))
pop.eps.4 <- rpareto(pop.n, location = 2, shape = 1) - 2
summary(pop.eps.4)
#+end_src

#+RESULTS:
:     Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
:      0.0      0.7      2.0     26.6      6.0 617400.0

Well, that's totally freaky. What about $\b$?

#+begin_src R :results output :exports both :tangle yes :session
pop.y.4 <- pop.X %*% beta + pop.eps.4
blist.4 <- t(sapply(1:draws, getb, pop.n = pop.n, pop.y = pop.y.4, pop.X = pop.X))
summary(blist.4)
#+end_src

#+RESULTS:
:        V1               V2
:  Min.   :-15.32   Min.   :-4.582
:  1st Qu.: 17.32   1st Qu.: 1.633
:  Median : 21.24   Median : 2.026
:  Mean   : 25.02   Mean   : 2.028
:  3rd Qu.: 26.84   3rd Qu.: 2.411
:  Max.   :129.29   Max.   : 9.162

A bit more reasonable. Does it look normal?

#+begin_src R :results output graphics :file inserts/fig4.png :width 800 :height 500 :session :tangle yes :exports both
(g3 <- make.plots(blist.4) + ggtitle("Pareto errors"))
#+end_src

#+RESULTS:
[[file:inserts/fig4.png]]
